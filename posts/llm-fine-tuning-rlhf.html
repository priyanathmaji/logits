<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Fine-tuning LLMs with RLHF - Priyanath Maji</title>
  <meta name="description" content="Deep dive into Reinforcement Learning from Human Feedback and how to apply it to improve LLM performance">
  <link rel="stylesheet" href="../styles.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <nav class="navbar">
    <div class="nav-container">
      <a href="../index.html" class="nav-brand">Priyanath Maji</a>
      <ul class="nav-menu">
        <li><a href="../index.html">Home</a></li>
        <li><a href="../blog.html" class="active">Blog</a></li>
      </ul>
      <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode">
        <span class="sun-icon">‚òÄÔ∏è</span>
        <span class="moon-icon">üåô</span>
      </button>
    </div>
  </nav>

  <main class="container">
    <article class="blog-post">
      <a href="../blog.html" class="back-link">‚Üê Back to Blog</a>
      
      <header class="post-header">
        <div class="post-meta">
          <span class="post-date">January 20, 2025</span>
          <span class="post-tag">Machine Learning</span>
        </div>
        <h1>Fine-tuning LLMs with RLHF: A Practical Guide</h1>
        <p style="color: var(--text-tertiary); font-style: italic;">Part 1 of LLM Mastery Series</p>
      </header>

      <div class="post-content">
        <h2>Introduction</h2>
        <p>Reinforcement Learning from Human Feedback (RLHF) has become the gold standard for aligning Large Language Models with human preferences. In this guide, I'll share practical insights from implementing RLHF in production for a wellness recommendation system that achieved a 38% increase in member enrollment.</p>

        <h2>What is RLHF?</h2>
        <p>RLHF is a technique that fine-tunes language models using human preferences as the reward signal. Unlike traditional supervised learning, RLHF optimizes for what humans actually want, not just what they say in labeled data.</p>

        <p>The process involves three key steps:</p>
        <ul>
          <li><strong>Supervised Fine-tuning (SFT):</strong> Train the base model on high-quality demonstrations</li>
          <li><strong>Reward Model Training:</strong> Train a model to predict human preferences</li>
          <li><strong>RL Optimization:</strong> Use PPO or similar algorithms to optimize the policy</li>
        </ul>

        <h2>The Mathematical Foundation</h2>
        <p>The core objective in RLHF is to maximize the expected reward while staying close to the original model. This is typically formulated as:</p>

        <p>$$
        \max_{\pi_\theta} \mathbb{E}_{x \sim D, y \sim \pi_\theta(y|x)} [r_\phi(x, y)] - \beta \mathbb{D}_{\text{KL}}[\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)]
        $$</p>

        <p>Where:</p>
        <ul>
          <li>$\pi_\theta$ is our policy (the LLM we're training)</li>
          <li>$r_\phi$ is the reward model</li>
          <li>$\beta$ controls how much we penalize deviation from the reference model</li>
          <li>$\mathbb{D}_{\text{KL}}$ is the KL divergence</li>
        </ul>

        <h2>Step 1: Collecting Human Preferences</h2>
        <p>The quality of your RLHF system heavily depends on the quality of your preference data. Here's what worked for our wellness recommendation system:</p>

        <pre><code class="language-python">import numpy as np
from dataclasses import dataclass
from typing import List, Tuple

@dataclass
class Preference:
    prompt: str
    chosen_response: str
    rejected_response: str
    confidence: float  # Annotator confidence 1-5

class PreferenceCollector:
    def __init__(self, model, num_samples=4):
        self.model = model
        self.num_samples = num_samples
    
    def generate_candidates(self, prompt: str) -> List[str]:
        """Generate multiple candidate responses"""
        responses = []
        for _ in range(self.num_samples):
            response = self.model.generate(
                prompt,
                temperature=0.8,
                max_tokens=512
            )
            responses.append(response)
        return responses
    
    def collect_preference(self, prompt: str) -> Preference:
        """Collect human preference between responses"""
        candidates = self.generate_candidates(prompt)
        
        # Present to human annotator
        chosen, rejected, confidence = self.get_human_feedback(
            prompt, candidates
        )
        
        return Preference(
            prompt=prompt,
            chosen_response=chosen,
            rejected_response=rejected,
            confidence=confidence
        )
</code></pre>

        <h2>Step 2: Training the Reward Model</h2>
        <p>The reward model learns to predict which response humans would prefer. We use a contrastive loss:</p>

        <p>$$
        \mathcal{L}_{\text{reward}} = -\log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))
        $$</p>

        <p>Where $y_w$ is the preferred (winning) response and $y_l$ is the less preferred (losing) response.</p>

        <pre><code class="language-python">import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class RewardModel(nn.Module):
    def __init__(self, base_model_name: str):
        super().__init__()
        self.transformer = AutoModel.from_pretrained(base_model_name)
        self.value_head = nn.Linear(
            self.transformer.config.hidden_size, 1
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # Use last token's hidden state
        last_hidden = outputs.last_hidden_state[:, -1, :]
        reward = self.value_head(last_hidden)
        return reward

def train_reward_model(preferences: List[Preference], epochs=3):
    model = RewardModel("gpt2-large")
    tokenizer = AutoTokenizer.from_pretrained("gpt2-large")
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    
    for epoch in range(epochs):
        total_loss = 0
        for pref in preferences:
            # Tokenize chosen and rejected responses
            chosen_tokens = tokenizer(
                pref.prompt + pref.chosen_response,
                return_tensors="pt"
            )
            rejected_tokens = tokenizer(
                pref.prompt + pref.rejected_response,
                return_tensors="pt"
            )
            
            # Get rewards
            r_chosen = model(**chosen_tokens)
            r_rejected = model(**rejected_tokens)
            
            # Compute loss
            loss = -torch.log(
                torch.sigmoid(r_chosen - r_rejected)
            )
            
            # Weight by confidence
            loss = loss * pref.confidence
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"Epoch {epoch}, Loss: {total_loss/len(preferences)}")
    
    return model
</code></pre>

        <h2>Step 3: PPO Training</h2>
        <p>Proximal Policy Optimization (PPO) is the most common algorithm for the RL phase. The key insight is to limit how much the policy can change in each update:</p>

        <pre><code class="language-python">from trl import PPOTrainer, PPOConfig
from transformers import AutoModelForCausalLM

def train_with_ppo(
    base_model_name: str,
    reward_model: RewardModel,
    prompts: List[str]
):
    # Initialize model and config
    model = AutoModelForCausalLM.from_pretrained(base_model_name)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    config = PPOConfig(
        model_name=base_model_name,
        learning_rate=1.41e-5,
        batch_size=32,
        mini_batch_size=4,
        gradient_accumulation_steps=1,
        ppo_epochs=4,
        gamma=1.0,  # No discounting
        lam=0.95,   # GAE lambda
        clip_range=0.2,
        clip_range_value=0.2,
        vf_coef=0.1,
    )
    
    trainer = PPOTrainer(
        config=config,
        model=model,
        ref_model=None,  # Will create reference model internally
        tokenizer=tokenizer,
        reward_model=reward_model
    )
    
    # Training loop
    for prompt in prompts:
        # Generate response
        prompt_tokens = tokenizer(prompt, return_tensors="pt")
        response_tokens = trainer.generate(
            **prompt_tokens,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.7
        )
        
        # Decode response
        response = tokenizer.decode(
            response_tokens[0], 
            skip_special_tokens=True
        )
        
        # Get reward
        reward = reward_model(response_tokens, prompt_tokens["attention_mask"])
        
        # PPO update
        stats = trainer.step([prompt_tokens], [response_tokens], [reward])
        
    return model
</code></pre>

        <h2>Practical Tips from Production</h2>
        
        <h3>1. Start Small, Scale Gradually</h3>
        <p>Don't try to fine-tune GPT-4 scale models immediately. Start with smaller models (7B parameters) and validate your pipeline:</p>
        <ul>
          <li>Use distillation from larger models for SFT data</li>
          <li>Validate reward model accuracy on held-out preferences</li>
          <li>Monitor KL divergence throughout training</li>
        </ul>

        <h3>2. Quality Over Quantity in Preferences</h3>
        <p>We found that 10,000 high-quality preferences from domain experts outperformed 100,000 crowdsourced preferences. Focus on:</p>
        <ul>
          <li>Clear preference distinctions</li>
          <li>Diverse prompt coverage</li>
          <li>Multiple annotators per comparison</li>
          <li>Regular annotator calibration</li>
        </ul>

        <h3>3. Monitor Reward Hacking</h3>
        <p>Models can exploit reward model weaknesses. Implement safeguards:</p>
        <pre><code class="language-python">def detect_reward_hacking(
    model, 
    reward_model, 
    test_prompts: List[str],
    threshold: float = 0.9
):
    """Detect if model is gaming the reward"""
    for prompt in test_prompts:
        response = model.generate(prompt)
        reward = reward_model(prompt, response)
        
        # Check for suspicious patterns
        if reward > threshold:
            # Manual review
            print(f"High reward ({reward}) for: {response}")
            
        # Check for repetition
        if check_repetition(response):
            print(f"Repetitive response: {response}")
            
        # Check for length gaming
        if len(response) > expected_length * 2:
            print(f"Unusually long response: {response}")
</code></pre>

        <h3>4. Use KL Penalty Wisely</h3>
        <p>The KL divergence penalty ($\beta$) is crucial. Too high and the model won't improve; too low and it becomes incoherent:</p>
        <pre><code class="language-python"># Start conservative, then decrease
beta_schedule = [0.1, 0.05, 0.02, 0.01]

for epoch, beta in enumerate(beta_schedule):
    train_epoch(model, beta=beta)
    
    # Evaluate coherence
    perplexity = evaluate_perplexity(model, validation_set)
    if perplexity > baseline_perplexity * 1.5:
        print(f"Warning: Perplexity increased to {perplexity}")
        break
</code></pre>

        <h2>Real-World Results</h2>
        <p>In our wellness recommendation system, RLHF fine-tuning delivered measurable improvements:</p>
        <ul>
          <li><strong>38% increase</strong> in member enrollment</li>
          <li><strong>45% reduction</strong> in inappropriate recommendations</li>
          <li><strong>92% accuracy</strong> in preference prediction</li>
          <li><strong>2.3x improvement</strong> in user satisfaction scores</li>
        </ul>

        <h2>Common Pitfalls to Avoid</h2>
        
        <h3>1. Insufficient SFT</h3>
        <p>Skip or rush supervised fine-tuning and your reward model won't have good candidates to choose from.</p>

        <h3>2. Reward Model Overfitting</h3>
        <p>Small preference datasets lead to overfit reward models. Use regularization and validation sets.</p>

        <h3>3. Ignoring Safety</h3>
        <p>RLHF can amplify biases. Implement safety filters and regular audits.</p>

        <h2>Next Steps</h2>
        <p>In Part 2 of this series, we'll explore how to combine RLHF with Retrieval-Augmented Generation (RAG) using vector databases for even better performance.</p>
        <img src="../images/rag-architecture.jpg" alt="RAG System Architecture">
        <h2>Resources</h2>
        <ul>
          <li><a href="https://arxiv.org/abs/2203.02155" target="_blank">InstructGPT Paper (OpenAI)</a></li>
          <li><a href="https://huggingface.co/blog/rlhf" target="_blank">HuggingFace RLHF Guide</a></li>
          <li><a href="https://github.com/lvwerra/trl" target="_blank">TRL Library (Transformer Reinforcement Learning)</a></li>
        </ul>

        <h2>Conclusion</h2>
        <p>RLHF is a powerful technique for aligning LLMs with human preferences, but it requires careful implementation. Start small, focus on quality data, and monitor for reward hacking. The results can be transformative for production applications.</p>
      </div>

      <footer class="post-footer">
        <div class="share-section">
          <h3>Share this post</h3>
          <div class="share-buttons">
            <a href="#" class="share-btn">Twitter</a>
            <a href="#" class="share-btn">LinkedIn</a>
            <a href="#" class="share-btn">Facebook</a>
          </div>
        </div>
      </footer>
    </article>
  </main>

  <script src="../script.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false}
        ],
        throwOnError: false
      });
    });
  </script>
</body>
</html>
