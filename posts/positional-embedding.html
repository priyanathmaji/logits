<!doctype html>
<html lang="en">
    
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Positional Embedding - logits</title>
  <meta name="description" content="Positional Embedding">
  <link rel="stylesheet" href="../styles.css" />
  <!-- KaTeX for Math -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
</head>
<body>
  <nav class="navbar">
    <div class="nav-container">
      <a href="../index.html" class="nav-brand">
        <img src="../images/logx.PNG" alt="logits">
        <span>logits</span>
      </a>
      <ul class="nav-menu">
        <li><a href="../index.html">Home</a></li>
        <li><a href="../blog.html" class="active">Blog</a></li>
      </ul>
      <button id="themeToggle" class="theme-toggle" aria-label="Toggle dark mode">
        <span class="sun-icon">‚òÄÔ∏è</span>
        <span class="moon-icon">üåô</span>
      </button>
    </div>
  </nav>

  <main class="blog-container">
    <article class="blog-post">
      <a href="../blog.html" class="back-link">‚Üê Back to Blog</a>
      
      <header class="post-header">
        <div class="post-meta">
          <span class="post-date">November 28, 2025</span>
          <span class="post-tag">Natural Language</span>
        </div>
        <h1>Positional Embedding</h1>
        <p style="color: var(--text-tertiary); font-style: italic;">Part 3 of LLM Mastery Series</p>
      </header>

      <div class="post-content">
<h2>Introduction</h2>
<p>Transformers treat input token as a set.</p>
<p>Attention has no built-in notion of order. This means<br><code>&quot;cat chased dog&quot; </code><br>and<br><code>&quot;dot chased cat&quot;</code><br>would look the same to the model without the positional info. Positional Embeddings add a sense of order to the token embedding vectors (content).</p>
<p><em><strong>&quot;We need a positional signal to help the model understand the effect of position of tokens.&quot;</strong></em><br><br></p>
<h2>Different methods of Positional embeddings</h2>
<ul>
<li><strong>Absolute</strong> sinusoidal (Vaswani et al. 2017)</li>
<li>Learned <strong>absolute</strong></li>
<li>RoPE (Rotary positional embedding) - LLaMa, GPT-J, Mistral</li>
</ul>
<br>

<h2>Intuition of RoPE</h2>
<p>In classic Transformer, position embedding is added to the token embedding, so all the vectors Query(Q), Key(K) and Value(V) inherit the positional info.</p>
<p>RoPE is a fixed positional embedding that encodes relative position than absolute position. <br><br>The difference between RoPE and absolute embeddings is:</p>
<p><span style="color: #5117d9;"><em>&quot;What matters is not the absolute positions of the tokens, but the distance between them.&quot;</em></span></p>
<br>



      </div>

      <footer class="post-footer">
        <div class="share-section">
          <h3>Share this post</h3>
          <div class="share-buttons">
            <a href="https://twitter.com/intent/tweet?text=Positional%20Embedding&url=https://priyanathmaji.github.io/logits/posts/positional-embedding.html" target="_blank" class="share-btn">Twitter</a>
            <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://priyanathmaji.github.io/logits/posts/positional-embedding.html" target="_blank" class="share-btn">LinkedIn</a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://priyanathmaji.github.io/logits/posts/positional-embedding.html" target="_blank" class="share-btn">Facebook</a>
          </div>
        </div>

        <!-- Disqus Comments -->
        <div id="disqus_thread"></div>
      </footer>
    </article>
  </main>

  <footer class="minimal-footer">
    <p>¬© 2025 logits. All rights reserved.</p>
  </footer>

  <script src="../script.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false}
        ],
        throwOnError: false
      });
    });
  </script>

  <!-- Disqus Configuration -->
  <script>
    var disqus_config = function () {
      this.page.url = 'https://priyanathmaji.github.io/logits/posts/positional-embedding.html';
      this.page.identifier = 'positional-embedding';
    };
    
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://https-priyanathmaji-github-io-logits.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>